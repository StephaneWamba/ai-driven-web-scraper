# AI-Driven Web Scraper Portfolio Project

> **ğŸš€ Portfolio Project**: AI-Powered Web Scraper for E-commerce Intelligence

A comprehensive full-stack application demonstrating AI-powered web scraping for competitive price intelligence across major e-commerce platforms.

## âœ¨ Features

- **ğŸ¤– AI-Powered Scraping**: Uses OpenAI GPT-4 for intelligent data extraction
- **ğŸ›’ Multi-Platform Support**: Amazon, Best Buy, Walmart
- **ğŸ“Š Real-Time Analytics**: Live price comparisons and market analysis
- **ğŸ¨ Modern UI**: React with TypeScript and Tailwind CSS
- **âš¡ High Performance**: FastAPI backend with async processing
- **â˜ï¸ Cloud-Native**: AWS infrastructure with Terraform
- **ğŸ”„ CI/CD Pipeline**: Automated deployment with GitHub Actions

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   React Frontendâ”‚    â”‚  FastAPI Backendâ”‚    â”‚  PostgreSQL DB  â”‚
â”‚   (TypeScript)  â”‚â—„â”€â”€â–ºâ”‚   (Python)      â”‚â—„â”€â”€â–ºâ”‚   (AWS RDS)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                       â”‚                       â”‚
         â”‚                       â”‚                       â”‚
         â–¼                       â–¼                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Nginx Proxy   â”‚    â”‚   AI Service    â”‚    â”‚   S3 Storage    â”‚
â”‚   (Port 80)     â”‚    â”‚  (OpenAI GPT-4) â”‚    â”‚   (Data Files)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Quick Start

### Prerequisites

- Python 3.11+
- Node.js 18+
- Docker & Docker Compose
- AWS CLI (for deployment)

### Local Development

```bash
# Clone the repository
git clone https://github.com/StephaneWamba/ai-driven-web-scraper.git
cd ai-driven-web-scraper

# Start with Docker Compose
docker-compose up -d

# Access the application
# Frontend: http://localhost
# Backend: http://localhost:8000
# API Docs: http://localhost:8000/docs
```

### Production Deployment

```bash
# Deploy infrastructure
cd terraform
terraform init
terraform apply -var="db_password=YourSecurePassword"

# Deploy application
./scripts/deploy.sh
```

## ğŸ“‹ Project Structure

```
ai-driven-web-scraper/
â”œâ”€â”€ backend/                 # FastAPI application
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ services/       # Business logic
â”‚   â”‚   â”œâ”€â”€ models.py       # Database models
â”‚   â”‚   â””â”€â”€ schemas.py      # Pydantic schemas
â”‚   â””â”€â”€ main.py             # Application entry point
â”œâ”€â”€ frontend/               # React TypeScript app
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ components/     # React components
â”‚   â”‚   â””â”€â”€ App.tsx         # Main application
â”‚   â””â”€â”€ package.json
â”œâ”€â”€ terraform/              # Infrastructure as Code
â”‚   â”œâ”€â”€ main.tf            # AWS resources
â”‚   â””â”€â”€ variables.tf       # Configuration
â”œâ”€â”€ scripts/               # Deployment scripts
â”œâ”€â”€ docs/                  # Documentation
â””â”€â”€ .github/workflows/     # CI/CD pipeline
```

## ğŸ”§ Configuration

### Environment Variables

```bash
# Backend (.env)
DATABASE_URL=postgresql://user:pass@host:5432/db
OPENAI_API_KEY=your_openai_api_key
AWS_REGION=us-east-1
S3_BUCKET_NAME=your-bucket-name

# Frontend
REACT_APP_API_URL=http://localhost:8000
```

### AWS Services Used

- **EC2**: Application server (t3.micro)
- **RDS**: PostgreSQL database
- **S3**: Data storage
- **VPC**: Network isolation
- **Security Groups**: Access control

## ğŸ§ª Testing

```bash
# Backend tests
cd backend
pip install -r requirements.txt
pytest

# Frontend tests
cd frontend
npm install
npm test
```

## ğŸ“Š Monitoring

### Health Checks

- **Backend**: `http://54.165.182.47:8000/health`
- **Frontend**: `http://54.165.182.47`

### Logs

```bash
# Backend logs
sudo journalctl -u ai-scraper-backend -f

# Nginx logs
sudo tail -f /var/log/nginx/access.log
```

## ğŸ”’ Security

- **HTTPS**: SSL/TLS encryption (configure with Let's Encrypt)
- **Authentication**: JWT-based auth (planned)
- **Input Validation**: Pydantic schemas
- **Rate Limiting**: API protection
- **Secrets Management**: GitHub Secrets for CI/CD

## ğŸ’° Cost Optimization

### Current Monthly Costs (~$30-40)

- **EC2 t3.micro**: ~$8-10
- **RDS t3.micro**: ~$15-20
- **S3 Storage**: ~$0.02/GB
- **Data Transfer**: Minimal

### Cost Reduction Tips

1. Use Spot Instances for non-critical workloads
2. Schedule shutdowns during off-hours
3. Monitor usage with CloudWatch
4. Use reserved instances for predictable workloads

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ†˜ Support

- **Documentation**: [docs/](docs/)
- **Issues**: [GitHub Issues](https://github.com/StephaneWamba/ai-driven-web-scraper/issues)
- **Discussions**: [GitHub Discussions](https://github.com/StephaneWamba/ai-driven-web-scraper/discussions)

---

**ğŸ¯ Perfect for**: Portfolio demonstrations, client presentations, learning modern full-stack development, and showcasing AI integration in web applications.

**ğŸ”— Repository**: [https://github.com/StephaneWamba/ai-driven-web-scraper](https://github.com/StephaneWamba/ai-driven-web-scraper)
